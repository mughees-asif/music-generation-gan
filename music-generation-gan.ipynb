{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9G8HOw2Cf9c"
   },
   "source": [
    "<h1 style='color:blue'><center>Music Generation using Symbolic Representation with Generative Adversarial Networks (GANs)</center></h1>\n",
    "\n",
    "---\n",
    "\n",
    "<b style='color:DodgerBlue'><center>Mughees Asif ‚Äì 180288337</center></b>\n",
    "<i style='color:rgb(0, 122, 172)'><center>ECS7022P - Computational Creativity</center></i>\n",
    "<i style='color:rgb(0, 122, 172)'><center>School of Electronic Engineering and Computer Science</center></i>\n",
    "<i style='color:rgb(0, 122, 172)'><center> Queen Mary, University of London</center></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDxKDCYtDW2J"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/mughees-asif/music-generation-gan/master/images/chopin_intro.jpg?token=GHSAT0AAAAAABS7Z352EPPBH2EJXFHK3NMCYSR24UQ\" alt=\"chopin\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TL5ZlEEIDSQJ"
   },
   "source": [
    "## Abstract\n",
    "\n",
    "Domain-style transfer for text, images, and music, has recently gained traction to enable\n",
    "creative and original digital content generation. The current trends in research for musical\n",
    "generation align with using deep neural networks due to the vast computational power\n",
    "and feasibility for producing varying styles of content. Following this light, the author\n",
    "has investigated and implemented the usage of Generative Adversarial Networks (GANs)\n",
    "with the Wasserstein Loss function to generate original musical snippets using symbolic\n",
    "notation. The dataset was sourced in a piano roll format, a widely used and accepted\n",
    "form of representing various musical tones and pitches. The GAN system was trained\n",
    "and tested in various configurations that show promising results. The generation of the\n",
    "output was aided by adding accompaniments that augment the original input samples\n",
    "with different tones and pitches to produce qualitatively pleasing music content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Note:</b> The proceeding notebook has been designed using techniques from the <a href='https://aws.amazon.com/deepcomposer/'>AWS Deepcomposer</a> samples <a href='https://stackoverflow.com/questions/17531684/n-grams-in-python-four-five-six-grams'>repository</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4673w7dGw2j"
   },
   "source": [
    "## Contents<a class=\"anchor\" id=\"contents\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "**1** &nbsp;&nbsp;**[Download packages](#dw-dep)**<br>\n",
    "**2** &nbsp;&nbsp;**[Import dependencies](#imports)**<br>\n",
    "**3** &nbsp;&nbsp;**[Data processing](#preprocess-data)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.1&nbsp;&nbsp;*[Output directories](#save-output)*<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.2&nbsp;&nbsp;*[Sample the data](#sample-data)*<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.3&nbsp;&nbsp;*[Data representation](#data-representation)*<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.4&nbsp;&nbsp;*[Prepare the data](#loaddata)*<br>\n",
    "**4** &nbsp;&nbsp;**[Generative Adversarial Network](#gan)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.1&nbsp;&nbsp;*[Generator](#generator)*<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4.2&nbsp;&nbsp;*[Discriminator](#discriminator)*<br>\n",
    "**5** &nbsp;&nbsp;**[Training](#training)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.1&nbsp;&nbsp;*[Optimiser](#optimiser)*<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.2&nbsp;&nbsp;*[Save checkpoints](#chkpt)*<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.3&nbsp;&nbsp;*[Loss](#loss)*<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.3.1&nbsp;&nbsp;*[Generator](#generator-train)*<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.3.2&nbsp;&nbsp;*[Discriminator](#discriminator-train)*<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5.4&nbsp;&nbsp;*[Experimentation](#testing)*<br>\n",
    "**6** &nbsp;&nbsp;**[Results](#results)**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.1&nbsp;&nbsp;*[Live](#live)*<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.2&nbsp;&nbsp;*[Recorded](#recorded)*<br>\n",
    "**7** &nbsp;&nbsp;**[References](#references)**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> To return to the contents, press the üîù icon.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjD4pbeJHJZn"
   },
   "source": [
    "## 1&nbsp;&nbsp;Download packages <a class=\"anchor\" id=\"dw-dep\"></a> [üîù](#contents)\n",
    "\n",
    "This section downloads the necessary packages needed to develop the GAN system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qxgXwJMSCbA4",
    "outputId": "9cab4f98-38e2-43b2-9164-208e15a25898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dependencies...\n",
      "Almost done...\n",
      "Dependencies downloaded and ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow==1.15.0 (from versions: 2.5.0rc0, 2.5.0rc1, 2.5.0rc2, 2.5.0rc3, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.8.0rc0, 2.8.0rc1, 2.8.0)\n",
      "ERROR: No matching distribution found for tensorflow==1.15.0\n"
     ]
    }
   ],
   "source": [
    "print('Downloading dependencies...')\n",
    "\n",
    "!pip install -q numpy\n",
    "!pip install -q scipy\n",
    "!pip install -q matplotlib\n",
    "!pip install -q ipython\n",
    "!pip install -q pypianoroll\n",
    "print('Almost done...')\n",
    "!pip install -q tensorflow==1.15.0\n",
    "\n",
    "print('Dependencies downloaded and ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxaGsA_dH2CF"
   },
   "source": [
    "## 2&nbsp;&nbsp;Import dependencies <a class=\"anchor\" id=\"imports\"></a> [üîù](#contents)\n",
    "\n",
    "This section imports the necessary dependencies needed to execute this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Warning:</b> The notebook relies on several utility functions that are present <a href='https://github.com/mughees-asif/music-generation-gan/tree/master/utils'>here</a>. Additionally, <b>ensure</b> to use the mentioned TensorFlow version, otherwise several dependency issues will arise.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TIvTp-4hIEF1",
    "outputId": "f38fa82b-4cce-49fa-8d7e-856401d2cebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing necessary dependencies...\n",
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "print('Importing necessary dependencies...')\n",
    "\n",
    "# Main imports\n",
    "import os \n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pypianoroll\n",
    "import scipy.stats\n",
    "import pickle\n",
    "import music21\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "# Utility imports \n",
    "from utils import display_utils, metrics_utils, path_utils, inference_utils, midi_utils\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('Ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQjSe0bHICae"
   },
   "source": [
    "## 3&nbsp;&nbsp;Data processing <a class=\"anchor\" id=\"preprocess-data\"></a> [üîù](#contents)\n",
    "\n",
    "This section will highlight the loading of the data for neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Warning:</b> The dataset in <code>NumPy</code> array style is available <a href='https://github.com/mughees-asif/music-generation-gan/tree/master/datasets'>here</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Output directories <a class=\"anchor\" id=\"save-output\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Setting up the folder structure to save the output...')\n",
    "\n",
    "root_dir = './output'\n",
    "\n",
    "# Save checkpoint generated during training\n",
    "check_dir = os.path.join(root_dir, 'load_saved')\n",
    "\n",
    "# Save MIDI files during training\n",
    "sample_dir = os.path.join(root_dir, 'saved_samples')\n",
    "\n",
    "# Create directories\n",
    "make_dir = [check_dir, sample_dir]\n",
    "for i in range(len(make_dir)):\n",
    "    os.makedirs(make_dir[i], exist_ok=True)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76LwrMQpnWR5"
   },
   "source": [
    "### 3.2 *Sample the data* <a class=\"anchor\" id=\"sample-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "53rkVH4ZIFNe",
    "outputId": "a6a00028-f75b-47b2-fb68-da45ce21b202"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv795'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv795');\n",
       "                               mp.base64Load('data:audio/midi;base64,TVRoZAAAAAYAAQAFBABNVHJrAAAAFAD/UQMJJ8AA/1gEBAIYCIgA/y8ATVRyawAAAFEA/wMHdW5rbm93bgDAAADgAEAAwACIAJBgZIgAgGAAAJBhZIgAgGEAAJBjZJAAgGMAAJBhZIgAgGEAAJBgZIgAgGAAAJBeZJAAgF4AiAD/LwBNVHJrAAAAYwD/Awd1bmtub3duAMAAAOAAQADAAIgAkFxkkACAXAAAkFtkiACAWwAAkFxkhACAXAAAkFtkhACAWwAAkFlkhACAWQAAkFtkhACAWwAAkFxkiACAXAAAkFtkkACAWwCIAP8vAE1UcmsAAABRAP8DB3Vua25vd24AwAAA4ABAAMAAiACQV2SIAIBXAACQUGSEAIBQAACQUmSEAIBSAACQVGSQAIBUAACQVWSIAIBVAACQV2SYAIBXAIgA/y8ATVRyawAAAFoA/wMHdW5rbm93bgDAAADgAEAAwACIAJBQZIgAgFAAAJBNZIgAgE0AAJBIZIgAgEgAAJBNZIgAgE0AAJBGZIgAgEYAAJBEZIgAgEQAAJBLZJAAgEsAiAD/LwA=');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Play a sample `MIDI` file to hear the original tones\n",
    "display_utils.playmidi('./original_midi/MIDI-0.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBc5lMSxnWR7"
   },
   "source": [
    "### 3.3 *Data representation* <a class=\"anchor\" id=\"data-representation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "ia4K2JaFnWR7",
    "outputId": "e53c4d0a-38bd-42cb-997d-533e5779ee96"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 265,
       "width": 884
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "training_data = np.load('./dataset/train.npy')\n",
    "\n",
    "# Display piano roll format of the training data\n",
    "display_utils.show_pianoroll(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0Lv3TmtnWR7"
   },
   "source": [
    "### 3.4 *Prepare the data* <a class=\"anchor\" id=\"load-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "L58WzPOVnWR8"
   },
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "filename = './dataset/train.npy'\n",
    "data = np.load(filename)\n",
    "data = np.asarray(data, dtype=np.float32)\n",
    "\n",
    "# Convert NumPy array into tensors for training\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "# Shuffle the dataset to add variablity \n",
    "dataset = dataset.shuffle(1000).repeat()\n",
    "\n",
    "# Init batch size\n",
    "dataset = dataset.batch(64, drop_remainder=True)\n",
    "\n",
    "# Reduce idle time by prefetching batches\n",
    "dataset = dataset.prefetch(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBRzTkK2nWR8"
   },
   "source": [
    "## 4&nbsp;&nbsp;Generative Adversarial Network (GAN)<a class=\"anchor\" id=\"gan\"></a> [üîù](#contents)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mughees-asif/music-generation-gan/master/images/gan.png?token=GHSAT0AAAAAABS7Z352VMPISW442TNVUA26YSZSEJA\" alt=\"gan\" style=\"width: 750px;\"/>\n",
    "<h5 align=\"center\"> Figure: GAN Architecture<sup>1</sup></h5>\n",
    "\n",
    "GANs are unsupervised neural network models that use two competing supervised sub-models (a generator $\\mathcal{G}$, and a discriminator $\\mathcal{D}$) to generate deep representations of the input data without the explicit need for annotations<sup>1</sup>. The two sub-models work in concert to develop high-dimensional distributions of the input data that generalises well on unseen samples.\n",
    "\n",
    "\\begin{align}\n",
    "    \\min_\\mathcal{G} \\max_\\mathcal{D} V(\\mathcal{G},\\mathcal{D}) = \\mathcal{E}_{x \\backsim p_{x}(x)}[\\log{\\mathcal{D}(x)}] +  \\mathcal{E}_{z \\backsim p_{z}(z)}[1-\\log{\\mathcal{D}(\\mathcal{G}(z))}]\n",
    "\\end{align}\n",
    "\n",
    "where:\n",
    "\n",
    "|  |  | \n",
    "| :--- | :--- | \n",
    "| $\\mathcal{D}(x)$ | represents the probability that $x$ is from the data and not $p_g$. | \n",
    "| $\\mathcal{E}_{x \\backsim p_{x}(x)}[\\log{\\mathcal{D}(x)}]$ | represents the probability that $x$ is from the data and not $p_g$. | \n",
    "| $\\mathcal{D}(\\mathcal{G}(z))$ | denotes $\\mathcal{D}$‚Äôs objective to correctly estimate the real data. | \n",
    "| $\\mathcal{D}(x)$ | signifies the probability that a sample is from $x$. | \n",
    "| $1-\\log{\\mathcal{D}(\\mathcal{G}(z))}$ | represents the syntheticism of $\\mathcal{D}(\\mathcal{G}(z))$. | \n",
    "| $\\mathcal{E}_{z \\backsim p_{z}(z)}[1-\\log{\\mathcal{D}(\\mathcal{G}(z))]}$ | denotes $\\mathcal{D}$‚Äôs objective to correctly estimate the probability of recognising the synthetic data. | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7N4TdgunWR8"
   },
   "source": [
    "### 4.1 *Generator* <a class=\"anchor\" id=\"generator\"></a>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/aws-samples/aws-deepcomposer-samples/ef66010dc65d9a38b5c8fcb2e7fdf9cde01ee066/gan/images/dgen.png\" alt=\"gan_generator\" style=\"width: 750px;\"/>\n",
    "<h5 align=\"center\"> Figure: The generator $\\mathcal{G}$ model based on the U-net architecture<sup>2</sup></h5>\n",
    "\n",
    "The generator $\\mathcal{G}$ \"$\\ldots$transforms a random noise vector into a synthetic sample, which resembles real samples drawn from a distribution of the real content\"<sup>3</sup>.\n",
    "\n",
    "The U-net architecture is an extension of the convolutional neural network (CNN) where pooling operations are used instead of upsampling operators, allowing for precise semantic segmentation and classification of an input image<sup>4</sup>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbNbdu0-nWR9",
    "outputId": "f79f86ab-388c-4bdd-ab7d-cb2e5568205b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Generator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 128, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 16, 64, 64)   1088        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)        (None, 16, 64, 64)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 8, 32, 128)   131200      ['leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)      (None, 8, 32, 128)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 8, 32, 128)  512         ['leaky_re_lu_1[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 4, 16, 256)   524544      ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)      (None, 4, 16, 256)   0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 4, 16, 256)  1024        ['leaky_re_lu_2[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 2, 8, 512)    2097664     ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)      (None, 2, 8, 512)    0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 2, 8, 512)   2048        ['leaky_re_lu_3[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 2, 8, 512)]  0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 2, 8, 1024)   0           ['batch_normalization_2[0][0]',  \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 4, 16, 1024)  0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 4, 16, 256)   4194560     ['up_sampling2d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 4, 16, 256)  1024        ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                   (None, 4, 16, 256)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4, 16, 256)   0           ['re_lu[0][0]']                  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 4, 16, 512)   0           ['dropout[0][0]',                \n",
      "                                                                  'batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 8, 32, 512)  0           ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 8, 32, 128)   1048704     ['up_sampling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 8, 32, 128)  512         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)                 (None, 8, 32, 128)   0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 8, 32, 128)   0           ['re_lu_1[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 8, 32, 256)   0           ['dropout_1[0][0]',              \n",
      "                                                                  'batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, 16, 64, 256)  0          ['concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 16, 64, 64)   262208      ['up_sampling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 16, 64, 64)  256         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)                 (None, 16, 64, 64)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 16, 64, 64)   0           ['re_lu_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 16, 64, 128)  0           ['dropout_2[0][0]',              \n",
      "                                                                  'leaky_re_lu[0][0]']            \n",
      "                                                                                                  \n",
      " up_sampling2d_3 (UpSampling2D)  (None, 32, 128, 128  0          ['concatenate_3[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 32, 128, 4)   8196        ['up_sampling2d_3[0][0]']        \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================\n",
      "Total params: 8,273,540\n",
      "Trainable params: 8,270,852\n",
      "Non-trainable params: 2,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "  def generator_(condition_input_shape=(32, 128, 1), filters=64,\n",
    "                    instruments=4, latent_shape=(2, 8, 512)):\n",
    "    \"\"\"\n",
    "    Builds and returns a generator model.\n",
    "    \"\"\"\n",
    "    c_input = tf.keras.layers.Input(shape=condition_input_shape)\n",
    "    z_input = tf.keras.layers.Input(shape=latent_shape)\n",
    "\n",
    "    #### Basic Downsampling Block ####\n",
    "    # d1\n",
    "    d1 = tf.keras.layers.Conv2D(filters, \n",
    "                               kernel_size=4, \n",
    "                               strides=2,\n",
    "                               padding='same')(c_input)\n",
    "    d1 = tf.keras.layers.LeakyReLU(alpha=0.2)(d1)\n",
    "\n",
    "    # d2\n",
    "    d2 = tf.keras.layers.Conv2D((filters * 2), \n",
    "                               kernel_size=4, \n",
    "                               strides=2,\n",
    "                               padding='same')(d1)\n",
    "    d2 = tf.keras.layers.LeakyReLU(alpha=0.2)(d2)\n",
    "    d2 = tf.keras.layers.BatchNormalization(momentum=0.8)(d2)\n",
    "    \n",
    "    # d3\n",
    "    d3 = tf.keras.layers.Conv2D((filters * 4), \n",
    "                               kernel_size=4, \n",
    "                               strides=2,\n",
    "                               padding='same')(d2)\n",
    "    d3 = tf.keras.layers.LeakyReLU(alpha=0.2)(d3)\n",
    "    d3 = tf.keras.layers.BatchNormalization(momentum=0.8)(d3)\n",
    "    \n",
    "    # d4\n",
    "    d4 = tf.keras.layers.Conv2D((filters * 8), \n",
    "                               kernel_size=4, \n",
    "                               strides=2,\n",
    "                               padding='same')(d3)\n",
    "    d4 = tf.keras.layers.LeakyReLU(alpha=0.2)(d4)\n",
    "    d4 = tf.keras.layers.BatchNormalization(momentum=0.8)(d4)\n",
    "    d4 = tf.keras.layers.Concatenate(axis=-1)([d4, z_input])\n",
    "    \n",
    "    #### Basic Upsampling Block ####\n",
    "    # u1\n",
    "    u1 = tf.keras.layers.UpSampling2D(size=2)(d4)\n",
    "    u1 = tf.keras.layers.Conv2D((filters * 4), \n",
    "                                kernel_size=4, \n",
    "                                strides=1,\n",
    "                                padding='same')(u1)\n",
    "    u1 = tf.keras.layers.BatchNormalization(momentum=0.8)(u1)\n",
    "    u1 = tf.keras.layers.ReLU()(u1)\n",
    "    u1 = tf.keras.layers.Dropout(0)(u1)\n",
    "    u1 = tf.keras.layers.Concatenate()([u1, d3])\n",
    "    \n",
    "    # u2\n",
    "    u2 = tf.keras.layers.UpSampling2D(size=2)(u1)\n",
    "    u2 = tf.keras.layers.Conv2D((filters * 2), \n",
    "                                kernel_size=4, \n",
    "                                strides=1,\n",
    "                                padding='same')(u2)\n",
    "    u2 = tf.keras.layers.BatchNormalization(momentum=0.8)(u2)\n",
    "    u2 = tf.keras.layers.ReLU()(u2)\n",
    "    u2 = tf.keras.layers.Dropout(0)(u2)\n",
    "    u2 = tf.keras.layers.Concatenate()([u2, d2])    \n",
    "    \n",
    "    # u3\n",
    "    u3 = tf.keras.layers.UpSampling2D(size=2)(u2)\n",
    "    u3 = tf.keras.layers.Conv2D((filters), \n",
    "                                kernel_size=4, \n",
    "                                strides=1,\n",
    "                                padding='same')(u3)\n",
    "    u3 = tf.keras.layers.BatchNormalization(momentum=0.8)(u3)\n",
    "    u3 = tf.keras.layers.ReLU()(u3)\n",
    "    u3 = tf.keras.layers.Dropout(0)(u3)\n",
    "    u3 = tf.keras.layers.Concatenate()([u3, d1])\n",
    "    \n",
    "    # u4\n",
    "    u4 = tf.keras.layers.UpSampling2D(size=2)(u3)\n",
    "    output = tf.keras.layers.Conv2D(instruments, \n",
    "                                    kernel_size=4, \n",
    "                                    strides=1,\n",
    "                                    padding='same', \n",
    "                                    activation='tanh')(u4)  # 32, 128, 4\n",
    "\n",
    "    generator = tf.keras.models.Model(\n",
    "        [c_input, z_input], \n",
    "        output, \n",
    "        name='Generator'\n",
    "    )\n",
    "\n",
    "    return generator\n",
    "\n",
    "# Initialise generator\n",
    "generator = generator_()\n",
    "# View summary of the model\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sx0IgEEPnWR9"
   },
   "source": [
    "### 4.2 *Discriminator* <a class=\"anchor\" id=\"discriminator\"></a>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/aws-samples/aws-deepcomposer-samples/ef66010dc65d9a38b5c8fcb2e7fdf9cde01ee066/gan/images/ddis.png\" alt=\"gan_discriminator\" style=\"width: 750px;\"/>\n",
    "<h5 align=\"center\"> Figure: The discriminator $\\mathcal{D}$ model based on the U-net architecture<sup>2</sup></h5>\n",
    "\n",
    "The discriminator $\\mathcal{D}$ \"$\\ldots$estimates the probability that a sample came from the real data rather than from the generator $\\mathcal{G}$\"<sup>3</sup>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdqGxBGMnWR9",
    "outputId": "d5c671bf-bb57-4bae-8c68-3c4304bda144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 32, 128, 4)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 32, 128, 1)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 32, 128, 5)   0           ['input_5[0][0]',                \n",
      "                                                                  'input_6[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 16, 64, 64)   5184        ['concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)      (None, 16, 64, 64)   0           ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 8, 32, 128)   131200      ['leaky_re_lu_6[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)      (None, 8, 32, 128)   0           ['conv2d_11[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 4, 16, 256)   524544      ['leaky_re_lu_7[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)      (None, 4, 16, 256)   0           ['conv2d_12[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 2, 8, 512)    2097664     ['leaky_re_lu_8[0][0]']          \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)      (None, 2, 8, 512)    0           ['conv2d_13[0][0]']              \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 8192)         0           ['leaky_re_lu_9[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            8193        ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,766,785\n",
      "Trainable params: 2,766,785\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def discriminator_(pianoroll_shape=(32, 128, 4), filters=64):\n",
    "    \"\"\"\n",
    "    Builds and returns a discriminator model.\n",
    "    \"\"\"\n",
    "    condition_input_shape = (32,128,1)\n",
    "    groundtruth_pianoroll = tf.keras.layers.Input(shape=pianoroll_shape)\n",
    "    condition_input = tf.keras.layers.Input(shape=condition_input_shape)\n",
    "    combined_imgs = tf.keras.layers.Concatenate(axis=-1)([groundtruth_pianoroll, condition_input])\n",
    "\n",
    "\n",
    "    # d1: Discriminator does not use batch-norm\n",
    "    d1 = tf.keras.layers.Conv2D(filters, \n",
    "                               kernel_size=4, \n",
    "                               strides=2,\n",
    "                               padding='same')(combined_imgs)\n",
    "    d1 = tf.keras.layers.LeakyReLU(alpha=0.2)(d1) \n",
    "    \n",
    "    # d2\n",
    "    d2 = tf.keras.layers.Conv2D((filters * 2), \n",
    "                               kernel_size=4, \n",
    "                               strides=2,\n",
    "                               padding='same')(d1)\n",
    "    d2 = tf.keras.layers.LeakyReLU(alpha=0.2)(d2) \n",
    "    \n",
    "    # d3\n",
    "    d3 = tf.keras.layers.Conv2D((filters * 4), \n",
    "                               kernel_size=4, \n",
    "                               strides=2,\n",
    "                               padding='same')(d2)\n",
    "    d3 = tf.keras.layers.LeakyReLU(alpha=0.2)(d3) \n",
    "    \n",
    "    # d4\n",
    "    d4 = tf.keras.layers.Conv2D((filters * 8), \n",
    "                               kernel_size=4, \n",
    "                               strides=2,\n",
    "                               padding='same')(d3)\n",
    "    d4 = tf.keras.layers.LeakyReLU(alpha=0.2)(d4) \n",
    "\n",
    "    x = tf.keras.layers.Flatten()(d4)\n",
    "    logit = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "    discriminator = tf.keras.models.Model(\n",
    "        [groundtruth_pianoroll, condition_input], \n",
    "        logit,                         \n",
    "        name='Discriminator')\n",
    "    \n",
    "    return discriminator\n",
    "\n",
    "# Initialise discriminator\n",
    "discriminator = discriminator_()\n",
    "# View summary of the model\n",
    "discriminator.summary() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcZuR_yynWR-"
   },
   "source": [
    "## 5&nbsp;&nbsp;Training <a class=\"anchor\" id=\"training\"></a> [üîù](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 *Optimiser* <a class=\"anchor\" id=\"optimiser\"></a>\n",
    "\n",
    "Training the models is executed by searching for the optimization of the objective function via the Adam algorithm<sup>5</sup>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "oas6QDuEnWR-"
   },
   "outputs": [],
   "source": [
    "# Setup Adam optimizers for both models\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-3, beta_1=0.5, beta_2=0.9)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-3, beta_1=0.5, beta_2=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 *Save checkpoints* <a class=\"anchor\" id=\"chkpt\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = tf.train.Checkpoint(\n",
    "    generator=generator,\n",
    "    generator_optimizer=generator_optimizer,\n",
    "    discriminator=discriminator,\n",
    "    iscriminator_optimizer=discriminator_optimizer\n",
    ")\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, check_dir, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 *Loss* <a class=\"anchor\" id=\"loss\"></a>\n",
    "\n",
    "Arjovsky et al. proposed the usage of a **Wasserstein loss function with gradient penalty** to improve the interpretability of the loss function and produce samples of higher quality<sup>6</sup>. The Wasserstein metric $\\mathcal{W}$ is a measure of the distance between two probability distributions and defined as:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathcal{W}(p_{z},p_{x}) = \\max_{\\parallel f \\parallel \\leq 1} \\mathcal{E}_{x \\backsim p_z}[f(x)] - \\mathcal{E}_{x \\backsim p_x}[f(x)]\n",
    "\\end{align}\n",
    "\n",
    "The gradient of the discriminator is *penalised* to ensure optimal differentiation between the real and synthetic samples<sup>6</sup>. This is achieved by implicitly defining $p_{\\hat{x}}$ which represents uniform sampling along straight lines between pair of points drawn from $p_z$ and $p_x$<sup>6</sup>. The resulting loss function $\\mathbb{L}$ can be parametrised to allow neural network training with two learnable functions for the discriminator $\\mathcal{D}_w$ and generator $\\mathcal{G}_{\\theta}$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbb{L}(p_{x}, p_{\\theta}, p_{\\hat{x}}) = \\max_{w \\in \\mathbb{W}} \\mathbb{E}_{x \\sim \\mathbb{p}_x}(\\mathcal{D}_w(x)) - \\mathbb{E}_{z \\sim p(z)}(\\mathcal{D}_w(\\mathcal{G}_{\\theta}(z)) + \\lambda \\mathbb{E}_{\\hat{x} \\sim \\mathbb{p}_\\hat{x}[(\\lVert \\nabla_{\\hat{x}}\\mathcal{D}_w(\\hat{x}) \\rVert_2 -  1)^2]}\n",
    "\\label{eq:1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 *Generator* <a class=\"anchor\" id=\"generator-train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "xPtoQs-bnWR_"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def generator_loss(x, condition_track_idx=0):\n",
    "    \"\"\"\n",
    "    Defines and returns the loss of the generator.\n",
    "    \"\"\"\n",
    "    # Real data \n",
    "    c = tf.expand_dims(x[..., condition_track_idx], -1)\n",
    "\n",
    "    # Latent vectors\n",
    "    z = tf.random.truncated_normal([64, 2, 8, 512])\n",
    "\n",
    "    # Update generator weights\n",
    "    with tf.GradientTape() as tape:\n",
    "        fake_x = generator((c, z), training=True)\n",
    "        fake_output = discriminator((fake_x,c), training=False)\n",
    "        \n",
    "        # -D(G(z|c))\n",
    "        ##############################################\n",
    "        # The generator trains to reduce the distance between \n",
    "        # the probability distribution of the synthetic  \n",
    "        # and the real samples.\n",
    "        ##############################################\n",
    "        gen_loss = -tf.reduce_mean(fake_output)\n",
    "\n",
    "    # Calculate gradients for Generator\n",
    "    gradients_of_generator = tape.gradient(gen_loss,\n",
    "                                           generator.trainable_variables)\n",
    "    # Update Generator\n",
    "    generator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_generator, generator.trainable_variables))\n",
    "\n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2 *Discriminator* <a class=\"anchor\" id=\"discriminator-train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "rkUazKjHnWR_"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def discriminator_loss(x, condition_track_idx=0):\n",
    "    \"\"\"\n",
    "    Defines and returns the loss of the discriminator.\n",
    "    \"\"\"\n",
    "    # Real data\n",
    "    c = tf.expand_dims(x[..., condition_track_idx], -1)\n",
    "\n",
    "    # Latent vectors\n",
    "    z = tf.random.truncated_normal([64, 2, 8, 512])\n",
    "\n",
    "    # Fake sample\n",
    "    fake_x = generator((c, z), training=False)\n",
    "\n",
    "    # Update discriminator weights\n",
    "    with tf.GradientTape() as tape:\n",
    "        real_output = discriminator((x, c), training=True)\n",
    "        fake_output = discriminator((fake_x, c), training=True)\n",
    "        \n",
    "        # D(G(z|c)) - D(x|c)\n",
    "        ##############################################\n",
    "        # The discriminator trains to find the distance between \n",
    "        # the probability distribution of the synthetic and \n",
    "        # the real samples.\n",
    "        ##############################################\n",
    "        discriminator_loss = tf.reduce_mean(real_output) - tf.reduce_mean(fake_output)\n",
    "\n",
    "    # Get the gradients for the real and synthetic samples\n",
    "    grads_of_discriminator = tape.gradient(discriminator_loss,\n",
    "                                               discriminator.trainable_variables)\n",
    "    \n",
    "    # Update discriminator weights based on gradient penalty loss\n",
    "    # maximize (D(x|c)) + (1 - D(G(z|c))|c) + (Gradient Penality) \n",
    "    with tf.GradientTape() as tape:\n",
    "        c1 = tf.expand_dims(x[..., 0], -1)\n",
    "        batch_size = x.get_shape().as_list()[0]\n",
    "        eps_x = tf.random.uniform(\n",
    "            [batch_size] + [1] * (len(x.get_shape()) - 1))  \n",
    "        inter = eps_x * x + (1.0 - eps_x) * fake_x\n",
    "\n",
    "        with tf.GradientTape() as g:\n",
    "            g.watch(inter)\n",
    "            disc_inter_output = discriminator((inter, c1), training=True)\n",
    "        grads = g.gradient(disc_inter_output, inter)\n",
    "        slopes = tf.sqrt(1e-8 + tf.reduce_sum(tf.square(grads), axis=0))\n",
    "        gp_loss = tf.reduce_mean(tf.square(slopes - 1.0))        \n",
    "        gp_loss *= 10.0\n",
    "\n",
    "    # Get the gradient penalty for both samples\n",
    "    grads_gp = tape.gradient(gp_loss, discriminator.trainable_variables)\n",
    "    gradients_of_discriminator = [g + ggp for g, ggp in\n",
    "                                  zip(grads_of_discriminator, grads_gp)\n",
    "                                  if ggp is not None]\n",
    "\n",
    "    # Update discriminator based on gradients\n",
    "    discriminator_optimizer.apply_gradients(\n",
    "        zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return (discriminator_loss + gp_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 *Experimentation* <a class=\"anchor\" id=\"testing\"></a>\n",
    "\n",
    "The two competing models, $\\mathcal{G}$ and $\\mathcal{D}$, run in a conditional loop which does not terminate until the defined number of iterations. The $\\mathcal{G}$ takes a sample from the real data and synthesizes several new multitrack piano roll samples with added accompaniments. The $\\mathcal{D}$ model takes the new samples and predicts how much tonal deviation exists between the real and synthetic samples. This information is fed back to the $\\mathcal{G}$, which updates the weights (using the Wasserstein Loss function with Gradient penalty) to bring the next synthetic samples closer to the real samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q-c-YpE_nWSA",
    "outputId": "4cee1c31-14c6-40b6-e98c-d350d51170d2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 melody samples\n",
      "Iteration #1, d_loss=-258764.88, g_loss=-238858.12\n",
      "Iteration #1, d_loss=-328907.69, g_loss=-342386.12\n",
      "Iteration #1, d_loss=-233976.50, g_loss=-300713.69\n",
      "Iteration #1, d_loss=-267617.62, g_loss=-257948.50\n",
      "Iteration #1, d_loss=-224109.56, g_loss=-221424.78\n",
      "Iteration #1, d_loss=-272556.12, g_loss=-227617.12\n",
      "Iteration #1, d_loss=-292313.38, g_loss=-271897.22\n",
      "Iteration #1, d_loss=-207756.38, g_loss=-265494.81\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-c9e0cfae2386>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m    \u001b[1;31m# Train discriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_dis_updates_per_gen_update\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;31m# Train generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\CC\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\CC\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\CC\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\miniconda3\\envs\\CC\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2958\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\CC\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1852\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\CC\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\CC\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load 10 input samples from the dataset and 10 random latent vectors\n",
    "sample_x, sample_z = inference_utils.load_melody_samples(n_sample=10)\n",
    "\n",
    "# Training iterations\n",
    "iterations = 1000\n",
    "\n",
    "# Update discriminator n times per generator update \n",
    "update_generator_per_discriminator_update = 5\n",
    "\n",
    "# Determine input track\n",
    "sample_c = tf.expand_dims(sample_x[..., 0], -1)\n",
    "\n",
    "# Intialise metric management\n",
    "metrics_utils.metrics_manager.initialize()\n",
    "\n",
    "# Save both losses\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "# Dataset iterator\n",
    "it = iter(dataset)\n",
    "\n",
    "# Training\n",
    "for iteration in range(iterations):\n",
    "    # Train discriminator\n",
    "    for _ in range(update_generator_per_discriminator_update):\n",
    "        # Generator loss\n",
    "        g_loss = generator_loss(next(it))\n",
    "        # Discriminator loss\n",
    "        d_loss = discriminator_loss(next(it))\n",
    "        \n",
    "        # Save Losses for visualisation\n",
    "        d_losses.append(d_loss)\n",
    "        g_losses.append(g_loss)\n",
    "\n",
    "        # Output training stats\n",
    "        if iteration % 20 == 0:\n",
    "            print('Iteration #{}, d_loss={:.2f}, g_loss={:.2f}'.format(iteration, d_loss, g_loss))\n",
    "\n",
    "        # Save checkpoints, music metrics, generated output\n",
    "        if iteration < 100 or iteration % 50 == 0 :\n",
    "            fake_sample_x = generator((sample_c, sample_z), training=False)\n",
    "            metrics_utils.metrics_manager.append_metrics_for_iteration(fake_sample_x.numpy(), iteration)\n",
    "\n",
    "            if iteration % 50 == 0:\n",
    "                ckpt_manager.save(checkpoint_number=iteration) \n",
    "\n",
    "                fake_sample_x = fake_sample_x.numpy()\n",
    "                # plot the pianoroll\n",
    "                display_utils.plot_pianoroll(iteration, sample_x[:4], fake_sample_x[:4], save_dir=sample_dir)\n",
    "                # generate the midi\n",
    "                destination_path = path_utils.generated_midi_path_for_iteration(iteration, saveto_dir=sample_dir)\n",
    "#                 midi_utils.save_pianoroll_as_midi(fake_sample_x[:4], destination_path=destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6&nbsp;&nbsp;Results <a class=\"anchor\" id=\"results\"></a> [üîù](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Live <a class=\"anchor\" id=\"live\"></a>\n",
    "\n",
    "The following output is from the above training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "id": "Ewf6IVZYA_2B",
    "outputId": "e4dd64a1-6a6e-419d-c20a-f6abb79a902d"
   },
   "outputs": [],
   "source": [
    "display_utils.plot_loss_logs(g_losses, d_losses, figsize=(15, 5), smoothing=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DeCHPtsOBFbM",
    "outputId": "c6dbac57-f1f9-47e7-8e73-26101d3ab5b2"
   },
   "outputs": [],
   "source": [
    "metrics_utils.metrics_manager.set_reference_metrics(training_data)\n",
    "metrics_utils.metrics_manager.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Recorded <a class=\"anchor\" id=\"recorded\"></a>\n",
    "\n",
    "The following results were recorded over the course of the training and displays the total $\\mathcal{G}$ \\& $\\mathcal{D}$ losses with various configurations of the training parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1000 iterations; no update to $\\mathcal{D}$ per $\\mathcal{G}$ output\n",
    "<img src=\"https://raw.githubusercontent.com/mughees-asif/music-generation-gan/master/images/1000_iters_no-update.png?token=GHSAT0AAAAAABS7Z353O24NYLVCPN3DJP2QYSR2TJQ\" alt=\"gan-train-1\" style=\"width: 750px;\"/>\n",
    "<img src=\"https://raw.githubusercontent.com/mughees-asif/music-generation-gan/master/images/1000_metrics_no-update.png?token=GHSAT0AAAAAABS7Z352E5V24WK4S3B3RY6MYSR22GQ\" alt=\"gan-metric-1\" style=\"width: 750px;\"/>\n",
    "\n",
    "#### 2000 iterations; 2 updates to $\\mathcal{D}$ per $\\mathcal{G}$ output\n",
    "<img src=\"https://raw.githubusercontent.com/mughees-asif/music-generation-gan/master/images/1000_iters_2-update.png?token=GHSAT0AAAAAABS7Z352FBKUFJ2SB23OACQAYSR2TYA\" alt=\"gan-train-2\" style=\"width: 750px;\"/>\n",
    "<img src=\"https://raw.githubusercontent.com/mughees-asif/music-generation-gan/master/images/1000_metrics_2-update.png?token=GHSAT0AAAAAABS7Z3527N3G34XJM267QVM2YSR22IQ\" alt=\"gan-metric-2\" style=\"width: 750px;\"/>\n",
    "\n",
    "#### 5000 iterations; 5 updates to $\\mathcal{D}$ per $\\mathcal{G}$ output\n",
    "<img src=\"https://raw.githubusercontent.com/mughees-asif/music-generation-gan/master/images/1000_iters_5-update.png?token=GHSAT0AAAAAABS7Z3532BIG434QMHKH75UKYSR2TVQ\" alt=\"gan-train-3\" style=\"width: 750px;\"/>\n",
    "<img src=\"https://raw.githubusercontent.com/mughees-asif/music-generation-gan/master/images/1000_metrics_5-update.png?token=GHSAT0AAAAAABS7Z353ZB4SW2QE6DRAJ4P2YSR22KA\" alt=\"gan-metric-3\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7&nbsp;&nbsp;References <a class=\"anchor\" id=\"references\"></a> [üîù](#contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sup>1</sup>Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A Bharath. ‚ÄúGenerative Adversarial Networks: An Overview‚Äù. In: *IEEE Signal Processing Magazine* 35.1 (2018), pp. 53‚Äì65.<br/>\n",
    "<sup>2</sup><a href='https://github.com/aws-samples/aws-deepcomposer-samples/tree/master/gan/images'>AWS Deepcomposer samples</a>.<br/>\n",
    "<sup>3</sup>Jean-Pierre Briot. ‚ÄúFrom artificial neural networks to deep learning for music generation: history, concepts and trends‚Äù. In: *Neural Computing and Applications 33.1* (2021), pp. 39‚Äì65.<br/>\n",
    "<sup>4</sup>Olaf Ronneberger, Philipp Fischer, and Thomas Brox. ‚ÄúU-net: Convolutional networks for biomedical image segmentation‚Äù. In: *International Conference on Medical Image Computing and Computer-Assisted Intervention*. Springer. 2015, pp. 234‚Äì241.<br/>\n",
    "<sup>5</sup>Kingma, D.P. and Ba, J., 2014. Adam: A method for stochastic optimization. *arXiv preprint arXiv:1412.6980*.<br/>\n",
    "<sup>6</sup>Martin Arjovsky, Soumith Chintala, and L√©on Bottou. ‚ÄúWasserstein Generative Adversarial Networks‚Äù. In: *Proceedings of the 34th International Conference on Machine Learning*. Ed. by Doina Precup and Yee Whye Teh. Vol. 70. Proceedings of Machine Learning Research. PMLR, June 2017, pp. 214‚Äì223. url: https://proceedings.mlr.press/v70/arjovsky17a.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:purple'><center><a href='#contents'>üîù</a>&#8592;&#8592;&#8592;&#8592; END &#8594;&#8594;&#8594;&#8594;<a href='#contents'>üîù</a></center></h1>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "music_generation_gan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
